# -*- coding: utf-8 -*-
"""PhÃ¢n_tÃ­ch_thÃ¡i_Ä‘á»™_cá»§a_sinh_viÃªn_UTC2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1W8YnDz-hWUiCO2QiZ-jUMY_Mg8DXoq8b

# **Äá»€ TÃ€I: NGHIÃŠN Cá»¨U TRIá»‚N KHAI KIáº¾N TRÃšC PHOBERT/VIBERT TRONG Xá»¬ LÃ NGÃ”N NGá»® Tá»° NHIÃŠN VÃ€ ÃP Dá»¤NG VÃ€O VIá»†C PHÃ‚N TÃCH THÃI Äá»˜ Cá»¦A NGÆ¯á»œI Há»ŒC Táº I UTC2**
"""

from google.colab import drive
drive.mount('/content/drive')

"""***CaÌ€i Ä‘ÄƒÌ£t thÆ° viÃªÌ£n***

"""

! pip install pyvi -qq

!pip install transformers==4.49.0 -qq

!pip install tensorflow -qq

!pip install keras -qq

!pip install sentencepiece -qq

!pip install vncorenlp -qq

"""## Data Loader"""

import numpy as np
import pandas as pd

train = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/data/train.xlsx', header = 0, names = ['sentiment', 'sentence'])
dev = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/data/valid.xlsx', header = 0, names = ['sentiment', 'sentence'])
test = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/data/test.xlsx', header = 0, names = ['sentiment', 'sentence'])

import pandas as pd
from sklearn.model_selection import train_test_split

# Äá»c dá»¯ liá»‡u Ä‘Ã£ gá»™p
data = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/combined_data_2.xlsx', header=0, names=['Sentiment', 'Sentences'])

# Chia dá»¯ liá»‡u thÃ nh train (70%), valid (15%), test (15%)
train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42, stratify=data['Sentiment'])
valid_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42, stratify=temp_data['Sentiment'])

# GÃ¡n vÃ o cÃ¡c biáº¿n
train = train_data
dev = valid_data
test = test_data

# In kÃ­ch thÆ°á»›c cá»§a cÃ¡c táº­p dá»¯ liá»‡u
print(f"Train size: {len(train)}, Validation size: {len(dev)}, Test size: {len(test)}")

from transformers import AutoTokenizer
import pandas as pd

# Táº£i tokenizer cá»§a PhoBERT
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2")

# Äá»c dá»¯ liá»‡u tá»« file Excel
data = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/combined_data_2.xlsx')

# HÃ m Ä‘áº¿m sá»‘ token trong cÃ¢u
def count_tokens(sentence):
    tokens = tokenizer.tokenize(sentence)
    return len(tokens)

# XÃ³a nhá»¯ng dÃ²ng cÃ³ sá»‘ token > 512
data = data[data['Sentences'].apply(count_tokens) <= 300]

# Kiá»ƒm tra káº¿t quáº£
print(data.head())
# LÆ°u DataFrame Ä‘Ã£ lá»c vÃ o file Excel má»›i
data.to_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/combined_data_2.xlsx', index=False)

import pandas as pd

# Äá»c dá»¯ liá»‡u tá»« file Excel (thay Ä‘á»•i Ä‘Æ°á»ng dáº«n náº¿u cáº§n)
data = pd.read_excel('/content/drive/MyDrive/NCKH_2425/DATASET/add_data/combined_data_2.xlsx')

# TÃ­nh Ä‘á»™ dÃ i cá»§a má»—i bÃ¬nh luáº­n (sá»‘ tá»« trong má»—i cÃ¢u)
data['Length'] = data['Sentences'].apply(lambda x: len(str(x).split()))

# TÃ­nh cÃ¡c thá»‘ng kÃª
average_length = data['Length'].mean()  # Äá»™ dÃ i trung bÃ¬nh
min_length = data['Length'].min()  # Äá»™ dÃ i ngáº¯n nháº¥t
max_length = data['Length'].max()  # Äá»™ dÃ i dÃ i nháº¥t

# In káº¿t quáº£
print(f"Äá»™ dÃ i trung bÃ¬nh: {average_length:.2f} tá»«")
print(f"Äá»™ dÃ i ngáº¯n nháº¥t: {min_length} tá»«")
print(f"Äá»™ dÃ i dÃ i nháº¥t: {max_length} tá»«")

# @title ID

from matplotlib import pyplot as plt
train['ID'].plot(kind='hist', bins=20, title='ID')
plt.gca().spines[['top', 'right',]].set_visible(False)

# @title Attitude

from matplotlib import pyplot as plt
import seaborn as sns
train.groupby('sentiment').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

# # @title Attitude vs ID

# from matplotlib import pyplot as plt
# import seaborn as sns
# figsize = (12, 1.2 * len(train['Sentiment'].unique()))
# plt.figure(figsize=figsize)
# sns.violinplot(train, x='ID', y='Sentiment', inner='box', palette='Dark2')
# sns.despine(top=True, right=True, bottom=True, left=True)

from vncorenlp import VnCoreNLP
from pyvi.ViTokenizer import ViTokenizer
vncorenlp = VnCoreNLP("vncorenlp/VnCoreNLP-1.1.1.jar", annotators="wseg", max_heap_size='-Xmx500m')

#pre-process
import re
import numpy as np

STOPWORDS = '/content/drive/MyDrive/NCKH_2425/DATASET/#dataVNstopwords/vietnamese-stopwords-dash.txt'
with open(STOPWORDS, "r") as ins:
    stopwords = []
    for line in ins:
        dd = line.strip('\n')
        stopwords.append(dd)
    stopwords = set(stopwords)

def filter_stop_words(train_sentences, stop_words):
    new_sent = [word for word in train_sentences.split() if word not in stop_words]
    train_sentences = ' '.join(new_sent)

    return train_sentences

def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

def preprocess(text, tokenized=True, lowercased=True):
    # text = ViTokenizer.tokenize(text)
    # text = ' '.join(vncorenlp.tokenize(text)[0])
    # text = filter_stop_words(text, stopwords)
    # text = deEmojify(text)
    # text = text.lower() if lowercased else text
    if tokenized:
        pre_text = ""
        sentences = vncorenlp.tokenize(text)
        for sentence in sentences:
            pre_text += " ".join(sentence)
        text = pre_text
    return text

def pre_process_features(X, y, tokenized=True, lowercased=True):
    X = [preprocess(str(p), tokenized=tokenized, lowercased=lowercased) for p in list(X)]
    for idx, ele in enumerate(X):
        if not ele:
            np.delete(X, idx)
            np.delete(y, idx)
    return X, y

X_train = train['sentence']
y_train = train['sentiment'].values
# y_train = train['Toxicity'].values

X_dev = dev['sentence']
y_dev = dev['sentiment'].values
# y_dev = dev['Toxicity'].values

X_test = test['sentence']
y_test = test['sentiment'].values
# y_test = test['Toxicity'].values


from sklearn.preprocessing import LabelEncoder

# Bien doi y bang label encoder thanh dang so
encoder = LabelEncoder()
encoder.fit(y_train)

y_train = encoder.transform(y_train)

encoder = LabelEncoder()
encoder.fit(y_dev)

y_dev = encoder.transform(y_dev)

encoder = LabelEncoder()
encoder.fit(y_test)

y_test = encoder.transform(y_test)

# Kiá»ƒm tra cÃ¡c giÃ¡ trá»‹ trong cá»™t 'Sentiment' cÃ³ pháº£i lÃ  float khÃ´ng
is_float = X_dev.apply(lambda x: isinstance(x, float))

# In ra cÃ¡c dÃ²ng cÃ³ giÃ¡ trá»‹ lÃ  float trong cá»™t 'Sentiment'
print(X_dev[is_float])

for i, sentence in enumerate(X_train):
    print(f"CÃ¢u {i+1}: {sentence}")

"""**###TEST TIá»€N Xá»¬ LÃ DL**"""

#preprocessing data
def Preprocess(string):
#   Remove cÃ¡c kÃ½ tá»± kÃ©o dÃ i: vd: Ä‘áº¹ppppppp
    string = re.sub(r'([A-Z])\1+', lambda m: m.group(1).upper(), string, flags=re.IGNORECASE)
#     viáº¿t thÆ°á»ng
    string = string.lower()
#     link
    string = re.sub('<.*?>', '', string).strip()
    string = re.sub('(\s)+', r'\1', string)
#     xÃ³a kÃ½ tá»± Ä‘áº·t biá»‡t
    string = re.sub(r"[-()\\\"#/@;:<>{}`+=~|.!?,%/]", "",string)
    string = re.sub('\n', ' ',string)
    string = re.sub('--', '',string)
    string = re.sub('  ', ' ',string)
    string = re.sub('   ', ' ',string)
    string = re.sub('    ', ' ',string)
#     xÃ³a sá»‘
    string = re.sub(r"\d+", "number", string)
#     xÃ³a
    string = re.sub("added.*photo", "", string)
    string = re.sub("added.*photos", "", string)
    string = re.sub("is.*post", "", string)
    string = re.sub("Photos.*post", "", string)
    string = re.sub("from.*post", "", string)
    string = re.sub("shared.*group", "", string)
    string = re.sub("shared.*post", "", string)
    string = re.sub("shared.*video", "", string)
    string = re.sub("is.*motivated", "", string)
    string = re.sub("is.*with", "", string)
    emoji_pattern = re.compile("["
                           u"\U0001F600-\U0001F64F"  # emoticons
                           u"\U0001F300-\U0001F5FF"  # symbols & pictographs
                           u"\U0001F680-\U0001F6FF"  # transport & map symbols
                           u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           u"\U00002702-\U000027B0"
                           u"\U000024C2-\U0001F251"
                           "]+", flags=re.UNICODE)
    string = emoji_pattern.sub(r'', string)
        # xá»­ lÃ½ emoj
    replace_list = {
                  # Positive
                  'ğŸ™‚': 'Positive', 'ğŸ˜€': 'Positive', 'ğŸ˜„': 'Positive', 'ğŸ˜†': 'Positive', 'ğŸ˜…': 'Positive', 'ğŸ˜‚': 'Positive',
                  'ğŸ˜Š': 'Positive', 'ğŸ˜Œ': 'Positive', 'ğŸ˜‰': 'Positive', 'ğŸ˜': 'Positive', 'ğŸ˜': 'Positive', 'ğŸ™ƒ': 'Positive',
                  'ğŸ˜º': 'Positive', 'ğŸƒ': 'Positive', 'ğŸ’©': 'Positive', 'ğŸ˜': 'Positive', 'ğŸ˜‹': 'Positive', 'ğŸ˜œ': 'Positive',
                  'ğŸ˜': 'Positive', 'ğŸ˜›': 'Positive', 'ğŸ˜ˆ': 'Positive', 'ğŸ˜‡': 'Positive', 'ğŸ˜¸': 'Positive', 'ğŸ˜¹': 'Positive',
                  'ğŸ˜¼': 'Positive', 'ğŸŒœ': 'Positive', 'ğŸŒ›': 'Positive', 'ğŸŒš': 'Positive', 'ğŸŒ': 'Positive', 'ğŸŒ': 'Positive',
                  'ğŸ‘': 'Positive', 'ğŸ‘Œ': 'Positive', 'âœŒ': 'Positive', 'ğŸ™Œ': 'Positive', 'ğŸ’¯': 'Positive', 'ğŸ™‹': 'Positive',
                  'âœ‹': 'Positive', 'âœ…': 'Positive', 'âœ”': 'Positive', 'ğŸ‘': 'Positive', 'ğŸ’ª': 'Positive',
                  'ğŸ™': 'Positive', 'â˜€': 'Positive', 'ğŸ‘‰': 'Positive', 'ğŸƒ': 'Positive', 'â˜': 'Positive',

                  # Negative
                  'ğŸ™': 'Negative', 'â˜¹': 'Negative', 'ğŸ˜': 'Negative', 'ğŸ˜–': 'Negative', 'ğŸ˜”': 'Negative', 'ğŸ˜“': 'Negative',
                  'ğŸ˜¢': 'Negative', 'ğŸ˜­': 'Negative', 'ğŸ˜Ÿ': 'Negative', 'ğŸ™': 'Negative', 'ğŸ˜¿': 'Negative',
                  'ğŸ˜°': 'Negative', 'ğŸ˜±': 'Negative', 'ğŸ™€': 'Negative', 'ğŸ˜§': 'Negative', 'ğŸ˜¨': 'Negative',

                  # Neutral
                  'ğŸ™„': 'Neutral', 'ğŸ’¥': 'Neutral', 'ğŸ˜²': 'Neutral', 'ğŸ˜³': 'Neutral'
              }




    for k, v in replace_list.items():
        string = string.replace(k, v)
#     remove ná»‘t nhá»¯ng kÃ½ tá»± thá»«a thÃ£i


    string = string.replace(u'"', u' ')
    string = string.replace(u'ï¸', u'')
    string = string.replace('ğŸ»','')

    return string

for i in range(len(X_train)):
    X_train.values[i]=Preprocess(X_train.values[i])

for i in range(len(X_dev)):
    X_dev.values[i]=Preprocess(X_dev.values[i])

for i in range(len(X_test)):
    X_test.values[i]=Preprocess(X_test.values[i])

dictionary = {}
with open("/content/drive/MyDrive/NCKH_2425/DATASET/tiengvietchuan.txt", "r") as file:
    for line in file:
        key, value = line.strip().split(",")
        key1 = str(key).strip()
        value1 = str(value).strip()
        dictionary[key1] = value1

print(dictionary)

def replace_string(text, dictionary):
    l = text.split(" ")
    for key in dictionary.keys():
        for item,va in enumerate(l):
            if va == key:
                l[item] = str(dictionary[key])
    result = " ".join(l)
    return result.strip()

for i in range(len(X_train)):
    X_train.values[i]=replace_string(X_train.values[i], dictionary)

for i in range(len(X_dev)):
    X_dev.values[i]=replace_string(X_dev.values[i], dictionary)

for i in range(len(X_test)):
    X_test.values[i]=replace_string(X_test.values[i], dictionary)

# Khá»Ÿi táº¡o 1 dict tá»•ng
all_results = {}

"""## phobert-base"""

import os
import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, precision_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import EarlyStoppingCallback, TrainerCallback
from accelerate import Accelerator

# Set environment variables
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # Make sure errors are reported immediately
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"

# Verify your GPU is available and working
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")

# Define label mapping (sá»‘ â†’ vÄƒn báº£n)
label_mapping = {
    0: "negative",
    1: "neutral",
    2: "positive"
}

# Let's make sure your data is properly preprocessed
# Giáº£ sá»­ pre_process_features tráº£ vá» nhÃ£n dáº¡ng sá»‘
train_X, train_y = pre_process_features(X_train, y_train, tokenized=True, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=True, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=True, lowercased=False)

# LÆ°u láº¡i báº£n sao cá»§a nhÃ£n sá»‘ Ä‘á»ƒ sá»­ dá»¥ng trong mÃ´ hÃ¬nh
train_y_numeric = train_y.copy()
dev_y_numeric = dev_y.copy()
test_y_numeric = test_y.copy()

# Check your labels to ensure they are in the correct format (0, 1, 2)
print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# Verify the labels are within the expected range
assert all(0 <= y < 3 for y in train_y), "Training labels must be 0, 1, or 2"
assert all(0 <= y < 3 for y in dev_y), "Dev labels must be 0, 1, or 2"
assert all(0 <= y < 3 for y in test_y), "Test labels must be 0, 1, or 2"

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained("vinai/phobert-base-v2", num_labels=3)
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2", use_fast=False)

# Create Dataset class
class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item

    def __len__(self):
        return len(self.labels)

# Tokenize with a smaller max_length to avoid memory issues
max_length = 100 # Reduced from 512
train_encodings = tokenizer(train_X, truncation=True, padding=True, max_length=max_length)
dev_encodings = tokenizer(dev_X, truncation=True, padding=True, max_length=max_length)
test_encodings = tokenizer(test_X, truncation=True, padding=True, max_length=max_length)

print("Label dtype:", train_y.dtype)
print("Label min:", train_y.min(), "Label max:", train_y.max())
print("Unique labels:", np.unique(train_y))

# Create datasets - sá»­ dá»¥ng nhÃ£n sá»‘ cho viá»‡c huáº¥n luyá»‡n
train_dataset = BuildDataset(train_encodings, train_y)
dev_dataset = BuildDataset(dev_encodings, dev_y)
test_dataset = BuildDataset(test_encodings, test_y)

# Enhanced callback to track train/val/test loss and accuracy
class LoggingCallback(TrainerCallback):
    def __init__(self, trainer_ref, test_dataset):
        self.trainer_ref = trainer_ref  # This will be set later
        self.test_dataset = test_dataset
        self.train_losses = []
        self.eval_losses = []
        self.test_losses = []
        self.eval_accuracies = []
        self.test_accuracies = []
        self.steps = []
        self.epochs = []

    def set_trainer(self, trainer):
        self.trainer_ref = trainer

    def on_train_begin(self, args, state, control, **kwargs):
        # This ensures the callback has a reference to the trainer
        if self.trainer_ref is None and 'trainer' in kwargs:
            self.trainer_ref = kwargs['trainer']

    def calculate_accuracy(self, predictions, true_labels):
        """Calculate accuracy from predictions and true labels"""
        pred_labels = np.argmax(predictions, axis=-1)
        return accuracy_score(true_labels, pred_labels)

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            # For training loss
            if "loss" in logs:
                self.train_losses.append(logs["loss"])
                self.steps.append(state.global_step)

            # For evaluation loss and accuracy (happens once per epoch)
            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
                self.epochs.append(state.epoch)

                # Calculate validation accuracy
                if self.trainer_ref is not None:
                    # Get validation predictions
                    val_outputs = self.trainer_ref.predict(self.trainer_ref.eval_dataset)
                    val_accuracy = self.calculate_accuracy(val_outputs.predictions, val_outputs.label_ids)
                    self.eval_accuracies.append(val_accuracy)

                    # Calculate test accuracy and loss
                    test_outputs = self.trainer_ref.predict(self.test_dataset)
                    test_loss = test_outputs.metrics["test_loss"]
                    test_accuracy = self.calculate_accuracy(test_outputs.predictions, test_outputs.label_ids)

                    self.test_losses.append(test_loss)
                    self.test_accuracies.append(test_accuracy)

                    print(f"Epoch {state.epoch:.2f}:")
                    print(f"  Validation - Loss: {logs['eval_loss']:.4f}, Accuracy: {val_accuracy:.4f}")
                    print(f"  Test - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}")

# Modify training arguments
training_args = TrainingArguments(
    output_dir='/content/drive/MyDrive/NCKH_2425/PhoBERT/PhoBERT_base',
    num_train_epochs=6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",  # Changed from eval_strategy
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,  # Lower eval_loss is better
    do_eval=True,
    no_cuda=False,
    fp16=False,  # Disable mixed precision to help with debugging
)

# Initialize our custom logging callback with None for trainer_ref
logging_callback = LoggingCallback(None, test_dataset)

# Initialize Trainer with the callbacks
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    callbacks=[logging_callback],
)

# Set trainer in callback after initialization
logging_callback.set_trainer(trainer)

# Dictionary to store metrics history
history = {
    'train_loss': [],
    'val_loss': [],
    'test_loss': [],
    'val_accuracy': [],
    'test_accuracy': []
}

accelerator = Accelerator()

# Train the model with error handling
try:
    # Train the model and collect training/validation loss at each epoch
    train_result = trainer.train()

    # Get training metrics
    metrics = train_result.metrics

    # Save model
    trainer.save_model("/content/drive/MyDrive/NCKH_2425/PhoBERT/PhoBERT_base")

    # Plot training and validation loss
    plt.figure(figsize=(15, 5))

    # Loss plot
    plt.subplot(1, 2, 1)
    plt.plot(logging_callback.steps, logging_callback.train_losses, label='Training Loss')

    # Check if we collected evaluation metrics
    if logging_callback.eval_losses and logging_callback.test_losses:
        # Calculate proper x-coordinates for eval and test losses
        eval_step_interval = max(1, len(logging_callback.steps) // len(logging_callback.eval_losses))
        eval_steps = [logging_callback.steps[min(i * eval_step_interval, len(logging_callback.steps) - 1)]
                      for i in range(len(logging_callback.eval_losses))]

        plt.plot(eval_steps, logging_callback.eval_losses, label='Validation Loss')
        plt.plot(eval_steps, logging_callback.test_losses, label='Test Loss')

    plt.xlabel('Steps')
    plt.ylabel('Loss')
    plt.title('Training, Validation, and Test Loss')
    plt.legend()
    plt.grid(True)

    # Accuracy plot
    plt.subplot(1, 2, 2)
    if logging_callback.eval_accuracies and logging_callback.test_accuracies:
        epochs = list(range(1, len(logging_callback.eval_accuracies) + 1))
        plt.plot(epochs, logging_callback.eval_accuracies, label='Validation Accuracy', marker='o')
        plt.plot(epochs, logging_callback.test_accuracies, label='Test Accuracy', marker='s')

        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.title('Validation and Test Accuracy per Epoch')
        plt.legend()
        plt.grid(True)

    plt.tight_layout()
    plt.savefig('training_metrics.png')
    plt.show()

    # Print accuracy summary
    if logging_callback.eval_accuracies and logging_callback.test_accuracies:
        print("\nAccuracy Summary:")
        print("=" * 50)
        for i, (val_acc, test_acc) in enumerate(zip(logging_callback.eval_accuracies, logging_callback.test_accuracies), 1):
            print(f"Epoch {i}: Validation Accuracy = {val_acc:.4f}, Test Accuracy = {test_acc:.4f}")

        print(f"\nBest Validation Accuracy: {max(logging_callback.eval_accuracies):.4f}")
        print(f"Best Test Accuracy: {max(logging_callback.test_accuracies):.4f}")

    # Evaluate on test set
    y_pred_classify = trainer.predict(test_dataset)
    y_pred_ids = np.argmax(y_pred_classify.predictions, axis=-1)

    # Chuyá»ƒn Ä‘á»•i nhÃ£n sá»‘ thÃ nh nhÃ£n vÄƒn báº£n
    y_pred_text = [label_mapping[idx] for idx in y_pred_ids]
    y_true_text = [label_mapping[idx] for idx in test_y]

    # Sá»­ dá»¥ng nhÃ£n sá»‘ cho tÃ­nh toÃ¡n cÃ¡c metrics
    y_true = test_y

    # Create a confusion matrix
    cf_matrix = confusion_matrix(y_true, y_pred_ids)
    plt.figure(figsize=(6, 4))  # Smaller size
    sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
               xticklabels=["negative", "neutral", "positive"],
               yticklabels=["negative", "neutral", "positive"])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.tight_layout()  # Ensure all elements fit in the figure
    plt.savefig('confusion_matrix.png')
    plt.show()

    # Print metrics
    acc = accuracy_score(y_true, y_pred_ids)
    print("Final Test Accuracy: {:.4f}".format(acc))

    f1_micro = f1_score(y_true, y_pred_ids, average='micro')
    print("F1 - micro: {:.4f}".format(f1_micro))

    precision = precision_score(y_true, y_pred_ids, average='macro')
    print("Precision (macro): {:.4f}".format(precision))

    recall = recall_score(y_true, y_pred_ids, average='macro')
    print("Recall (macro): {:.4f}".format(recall))

    # Plot class-wise metrics
    class_names = ["negative", "neutral", "positive"]
    class_precision = precision_score(y_true, y_pred_ids, average=None)
    class_recall = recall_score(y_true, y_pred_ids, average=None)
    class_f1 = f1_score(y_true, y_pred_ids, average=None)

    plt.figure(figsize=(8, 6))
    x = np.arange(len(class_names))
    width = 0.25

    plt.bar(x - width, class_precision, width, label='Precision')
    plt.bar(x, class_recall, width, label='Recall')
    plt.bar(x + width, class_f1, width, label='F1-score')

    plt.xlabel('Classes')
    plt.ylabel('Score')
    plt.title('Class-wise Performance Metrics')
    plt.xticks(x, class_names)
    plt.legend()
    plt.tight_layout()
    plt.savefig('class_metrics.png')
    plt.show()

    model_name = "PhoBert"
    metrics = {
        "accuracy": acc,
        "f1_micro": f1_micro,
        "precision": precision,
        "recall": recall
    }
    all_results[model_name] = metrics

    # Print some example predictions with text labels
    print("\nExample predictions:")
    print("------------------------")
    for i in range(min(10, len(test_X))):
        print(f"Text: {test_X[i][:50]}...")
        print(f"True sentiment: {y_true_text[i]}")
        print(f"Predicted sentiment: {y_pred_text[i]}")
        print("------------------------")

except RuntimeError as e:
    print(f"Runtime error encountered: {e}")

    # Print GPU memory usage for debugging
    if torch.cuda.is_available():
        print(f"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
        print(f"GPU memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB")
        torch.cuda.empty_cache()  # Free up GPU memory

"""# Bert4news"""

# Bert4news - Training on Colab with Per-Epoch Accuracy Tracking

# 1. Import libraries
import os
import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, precision_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import EarlyStoppingCallback, TrainerCallback, set_seed

# 2. Setup
# Set environment variables for better logging
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"

# Set random seed
set_seed(42)

# Label mapping
label_mapping = {0: "negative", 1: "neutral", 2: "positive"}

# Model & output directory (for Colab)
base_model_dir = "/content/drive/MyDrive/NCKH_2425/ViBert4News/Base"

# 3. Preprocess data
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# 4. Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(base_model_dir, num_labels=3)
tokenizer = AutoTokenizer.from_pretrained(base_model_dir, use_fast=True)

# 5. Dataset class
class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Tokenize data
train_encodings = tokenizer(train_X, truncation=True, padding=True, max_length=512)
dev_encodings = tokenizer(dev_X, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_X, truncation=True, padding=True, max_length=512)

# Create datasets
train_dataset = BuildDataset(train_encodings, train_y)
dev_dataset = BuildDataset(dev_encodings, dev_y)
test_dataset = BuildDataset(test_encodings, test_y)

# 6. Enhanced Loss and Accuracy Tracking Callback
class LossTrackingCallback(TrainerCallback):
    def __init__(self):
        self.train_losses = []
        self.eval_losses = []
        self.test_losses = []
        self.eval_accuracies = []
        self.test_accuracies = []
        self.steps = []
        self.epochs = []
        self.trainer = None
        self.test_dataset = None

    def on_train_begin(self, args, state, control, **kwargs):
        if 'trainer' in kwargs:
            self.trainer = kwargs['trainer']

    def set_trainer_and_dataset(self, trainer, test_dataset):
        self.trainer = trainer
        self.test_dataset = test_dataset

    def calculate_accuracy(self, predictions, true_labels):
        """Calculate accuracy from predictions and true labels"""
        pred_labels = np.argmax(predictions, axis=-1)
        return accuracy_score(true_labels, pred_labels)

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if "loss" in logs:
                self.train_losses.append(logs["loss"])
                self.steps.append(state.global_step)

            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
                self.epochs.append(state.epoch)

                if self.trainer is not None and self.test_dataset is not None:
                    # Calculate validation accuracy
                    val_outputs = self.trainer.predict(self.trainer.eval_dataset)
                    val_accuracy = self.calculate_accuracy(val_outputs.predictions, val_outputs.label_ids)
                    self.eval_accuracies.append(val_accuracy)

                    # Calculate test accuracy and loss
                    test_outputs = self.trainer.predict(self.test_dataset)
                    test_loss = test_outputs.metrics["test_loss"]
                    test_accuracy = self.calculate_accuracy(test_outputs.predictions, test_outputs.label_ids)

                    self.test_losses.append(test_loss)
                    self.test_accuracies.append(test_accuracy)

                    print(f"Epoch {state.epoch:.2f}:")
                    print(f"  Validation - Loss: {logs['eval_loss']:.4f}, Accuracy: {val_accuracy:.4f}")
                    print(f"  Test - Loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}")

# 7. Training arguments
training_args = TrainingArguments(
    output_dir=base_model_dir,
    num_train_epochs=6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=3e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    no_cuda=False,
    do_eval=True
)

# Early stopping
early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=4)

# Initialize loss tracker
loss_tracker = LossTrackingCallback()

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    callbacks=[early_stopping_callback, loss_tracker],
)

loss_tracker.set_trainer_and_dataset(trainer, test_dataset)

# 8. Train model
print("Starting training...")
trainer.train()
trainer.save_model(base_model_dir)
print("Training completed and model saved.")

# 9. Plot training, validation, and test loss & accuracy
plt.figure(figsize=(15, 5))

# Loss plot
plt.subplot(1, 2, 1)
plt.plot(loss_tracker.steps, loss_tracker.train_losses, label='Training Loss')

if loss_tracker.eval_losses and loss_tracker.test_losses:
    eval_step_interval = max(1, len(loss_tracker.steps) // len(loss_tracker.eval_losses))
    eval_steps = [loss_tracker.steps[min(i * eval_step_interval, len(loss_tracker.steps) - 1)] for i in range(len(loss_tracker.eval_losses))]
    plt.plot(eval_steps, loss_tracker.eval_losses, label='Validation Loss')
    plt.plot(eval_steps, loss_tracker.test_losses, label='Test Loss')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training, Validation, and Test Loss')
plt.legend()
plt.grid(True)

# Accuracy plot
plt.subplot(1, 2, 2)
if loss_tracker.eval_accuracies and loss_tracker.test_accuracies:
    epochs = list(range(1, len(loss_tracker.eval_accuracies) + 1))
    plt.plot(epochs, loss_tracker.eval_accuracies, label='Validation Accuracy', marker='o')
    plt.plot(epochs, loss_tracker.test_accuracies, label='Test Accuracy', marker='s')

    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Validation and Test Accuracy per Epoch')
    plt.legend()
    plt.grid(True)

plt.tight_layout()
plt.savefig('vibert4news_training_metrics.png')
plt.show()

# Print accuracy summary
if loss_tracker.eval_accuracies and loss_tracker.test_accuracies:
    print("\nAccuracy Summary:")
    print("=" * 50)
    for i, (val_acc, test_acc) in enumerate(zip(loss_tracker.eval_accuracies, loss_tracker.test_accuracies), 1):
        print(f"Epoch {i}: Validation Accuracy = {val_acc:.4f}, Test Accuracy = {test_acc:.4f}")

    print(f"\nBest Validation Accuracy: {max(loss_tracker.eval_accuracies):.4f}")
    print(f"Best Test Accuracy: {max(loss_tracker.test_accuracies):.4f}")

# 10. Evaluate on test set
y_pred_classify = trainer.predict(test_dataset)
y_pred_ids = np.argmax(y_pred_classify.predictions, axis=-1)

# Map IDs to text labels
y_pred_text = [label_mapping[idx] for idx in y_pred_ids]
y_true_text = [label_mapping[idx] for idx in test_y]

# Metrics
y_true = test_y

# Confusion Matrix
cf_matrix = confusion_matrix(y_true, y_pred_ids)
plt.figure(figsize=(6, 4))
sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["negative", "neutral", "positive"],
           yticklabels=["negative", "neutral", "positive"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('vibert4news_confusion_matrix.png')
plt.show()

# Scores
acc = accuracy_score(y_true, y_pred_ids)
print("Final Test Accuracy: {:.4f}".format(acc))

f1_micro = f1_score(y_true, y_pred_ids, average='micro')
print("F1 - micro: {:.4f}".format(f1_micro))

precision = precision_score(y_true, y_pred_ids, average='macro')
print("Precision (macro): {:.4f}".format(precision))

recall = recall_score(y_true, y_pred_ids, average='macro')
print("Recall (macro): {:.4f}".format(recall))

# 11. Plot class-wise metrics
class_names = ["negative", "neutral", "positive"]
class_precision = precision_score(y_true, y_pred_ids, average=None)
class_recall = recall_score(y_true, y_pred_ids, average=None)
class_f1 = f1_score(y_true, y_pred_ids, average=None)

plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Precision')
plt.bar(x, class_recall, width, label='Recall')
plt.bar(x + width, class_f1, width, label='F1-score')

plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Class-wise Performance Metrics')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('vibert4news_class_metrics.png')
plt.show()

model_name = "ViBert4News"
metrics = {
    "accuracy": acc,
    "f1_micro": f1_micro,
    "precision": precision,
    "recall": recall
}
all_results[model_name] = metrics

# 12. Print some example predictions
print("\nExample predictions:")
print("------------------------")
for i in range(min(10, len(test_X))):
    print(f"Text: {test_X[i][:50]}...")
    print(f"True sentiment: {y_true_text[i]}")
    print(f"Predicted sentiment: {y_pred_text[i]}")
    print("------------------------")

"""## vibert-base-cased"""

# Bert4news - Training on Colab

# 1. Import libraries
import os
import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, precision_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import EarlyStoppingCallback, TrainerCallback, set_seed

# 2. Setup
# Set environment variables for better logging
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"

# Set random seed
set_seed(42)

# Label mapping
label_mapping = {0: "negative", 1: "neutral", 2: "positive"}

# Model & output directory (for Colab)
base_model_dir = "/content/drive/MyDrive/NCKH_2425/ViBert/Base"

# 3. Preprocess data
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# 4. Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(base_model_dir, num_labels=3)
tokenizer = AutoTokenizer.from_pretrained(base_model_dir, use_fast=True)

# 5. Dataset class
class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Tokenize data
train_encodings = tokenizer(train_X, truncation=True, padding=True, max_length=512)
dev_encodings = tokenizer(dev_X, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_X, truncation=True, padding=True, max_length=512)

# Create datasets
train_dataset = BuildDataset(train_encodings, train_y)
dev_dataset = BuildDataset(dev_encodings, dev_y)
test_dataset = BuildDataset(test_encodings, test_y)

# 6. Custom Loss Tracking Callback
class LossTrackingCallback(TrainerCallback):
    def __init__(self):
        self.train_losses = []
        self.eval_losses = []
        self.test_losses = []
        self.steps = []
        self.epochs = []
        self.trainer = None
        self.test_dataset = None

    def on_train_begin(self, args, state, control, **kwargs):
        if 'trainer' in kwargs:
            self.trainer = kwargs['trainer']

    def set_trainer_and_dataset(self, trainer, test_dataset):
        self.trainer = trainer
        self.test_dataset = test_dataset

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if "loss" in logs:
                self.train_losses.append(logs["loss"])
                self.steps.append(state.global_step)
            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
                if self.trainer is not None and self.test_dataset is not None:
                    test_outputs = self.trainer.predict(self.test_dataset)
                    test_loss = test_outputs.metrics["test_loss"]
                    self.test_losses.append(test_loss)
                    self.epochs.append(state.epoch)
                    print(f"Epoch {state.epoch:.2f}: Validation loss: {logs['eval_loss']:.4f}, Test loss: {test_loss:.4f}")

# 7. Training arguments
training_args = TrainingArguments(
    output_dir=base_model_dir,
    num_train_epochs=6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=3e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    no_cuda=False,
    do_eval=True
)

# Early stopping
early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)

# Initialize loss tracker
loss_tracker = LossTrackingCallback()

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    callbacks=[early_stopping_callback, loss_tracker],
)

loss_tracker.set_trainer_and_dataset(trainer, test_dataset)

# 8. Train model
print("Starting training...")
trainer.train()
trainer.save_model(base_model_dir)
print("Training completed and model saved.")

# 9. Plot training, validation, and test loss
plt.figure(figsize=(10, 6))
plt.plot(loss_tracker.steps, loss_tracker.train_losses, label='Training Loss')

if loss_tracker.eval_losses and loss_tracker.test_losses:
    eval_step_interval = max(1, len(loss_tracker.steps) // len(loss_tracker.eval_losses))
    eval_steps = [loss_tracker.steps[min(i * eval_step_interval, len(loss_tracker.steps) - 1)] for i in range(len(loss_tracker.eval_losses))]
    plt.plot(eval_steps, loss_tracker.eval_losses, label='Validation Loss')
    plt.plot(eval_steps, loss_tracker.test_losses, label='Test Loss')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training, Validation, and Test Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curves.png')
plt.show()

# 10. Evaluate on test set
y_pred_classify = trainer.predict(test_dataset)
y_pred_ids = np.argmax(y_pred_classify.predictions, axis=-1)

# Map IDs to text labels
y_pred_text = [label_mapping[idx] for idx in y_pred_ids]
y_true_text = [label_mapping[idx] for idx in test_y]

# Metrics
y_true = test_y

# Confusion Matrix
cf_matrix = confusion_matrix(y_true, y_pred_ids)
plt.figure(figsize=(6, 4))
sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["negative", "neutral", "positive"],
           yticklabels=["negative", "neutral", "positive"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

# Scores
acc = accuracy_score(y_true, y_pred_ids)
print("Accuracy: {:.4f}".format(acc))

f1_micro = f1_score(y_true, y_pred_ids, average='micro')
print("F1 - micro: {:.4f}".format(f1_micro))

precision = precision_score(y_true, y_pred_ids, average='macro')
print("Precision (micro): {:.4f}".format(precision))

recall = recall_score(y_true, y_pred_ids, average='macro')
print("Recall (micro): {:.4f}".format(recall))

# 11. Plot class-wise metrics
class_names = ["negative", "neutral", "positive"]
class_precision = precision_score(y_true, y_pred_ids, average=None)
class_recall = recall_score(y_true, y_pred_ids, average=None)
class_f1 = f1_score(y_true, y_pred_ids, average=None)

plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Precision')
plt.bar(x, class_recall, width, label='Recall')
plt.bar(x + width, class_f1, width, label='F1-score')

plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Class-wise Performance Metrics')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('vibert4news_class_metrics.png')
plt.show()


model_name = "ViBert"
metrics = {
    "accuracy": acc,
    "f1_micro": f1_micro,
    "precision": precision,
    "recall": recall
}
all_results[model_name] = metrics

# 12. Print some example predictions
print("\nExample predictions:")
print("------------------------")
for i in range(min(10, len(test_X))):
    print(f"Text: {test_X[i][:50]}...")
    print(f"True sentiment: {y_true_text[i]}")
    print(f"Predicted sentiment: {y_pred_text[i]}")
    print("------------------------")

"""# XLM-RoBEARTa"""

# XLM-RoBERTa training full code

import os
import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, precision_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback

# Set environment variables
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"

# Preprocess data
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

# Label distribution
print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(
    "xlm-roberta-base",
    num_labels=3
)
tokenizer = AutoTokenizer.from_pretrained(
    "xlm-roberta-base",
    use_fast=True
)

# Dataset class
class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Tokenize data
train_encodings = tokenizer(train_X, truncation=True, padding=True, max_length=512)
dev_encodings = tokenizer(dev_X, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_X, truncation=True, padding=True, max_length=512)

# Create datasets
train_dataset = BuildDataset(train_encodings, train_y)
dev_dataset = BuildDataset(dev_encodings, dev_y)
test_dataset = BuildDataset(test_encodings, test_y)

# Custom callback to track train/val/test loss
class LossTrackingCallback(TrainerCallback):
    def __init__(self, trainer, test_dataset):
        self.trainer = trainer
        self.test_dataset = test_dataset
        self.train_losses = []
        self.eval_losses = []
        self.test_losses = []
        self.steps = []
        self.epochs = []

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None and "loss" in logs:
            self.train_losses.append(logs["loss"])
            self.steps.append(state.global_step)

    def on_evaluate(self, args, state, control, metrics, **kwargs):
        if "eval_loss" in metrics:
            self.eval_losses.append(metrics["eval_loss"])
            self.epochs.append(state.epoch)

            # Predict test loss
            test_outputs = self.trainer.predict(self.test_dataset)
            test_loss = test_outputs.metrics.get("eval_loss", None)
            if test_loss is not None:
                self.test_losses.append(test_loss)
            else:
                self.test_losses.append(-1)

# Training arguments
training_args = TrainingArguments(
    output_dir='/content/drive/MyDrive/NCKH_2425/XLM-RoBERTa',
    num_train_epochs=6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=2e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    no_cuda=False,
    do_eval=True
)

# Callbacks
early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    callbacks=[early_stopping_callback]
)

# Add loss tracking
loss_tracker = LossTrackingCallback(trainer, test_dataset)
trainer.add_callback(loss_tracker)

# Train model
print("Starting XLM-RoBERTa training...")
trainer.train()
trainer.save_model("/content/drive/MyDrive/NCKH_2425/XLM-RoBERTa")
print("Training completed and model saved.")

# Plot loss curves
plt.figure(figsize=(10, 6))
plt.plot(loss_tracker.steps, loss_tracker.train_losses, label='Training Loss')
if loss_tracker.eval_losses:
    eval_steps = [max(s for s in loss_tracker.steps if s <= (e * len(loss_tracker.steps)/6))
                  for e in range(1, len(loss_tracker.eval_losses) + 1)]
    plt.plot(eval_steps, loss_tracker.eval_losses, label='Validation Loss')
    plt.plot(eval_steps, loss_tracker.test_losses, label='Test Loss')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training, Validation, and Test Loss')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('xlm_roberta_loss_curves.png')
plt.show()

# Evaluate on test set
print("Evaluating on test set...")
y_pred_classify = trainer.predict(test_dataset)
y_pred = np.argmax(y_pred_classify.predictions, axis=-1)
y_true = test_y

# Confusion matrix
cf_matrix = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(6, 4))
sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["Negative", "Neutral", "Positive"],
           yticklabels=["Negative", "Neutral", "Positive"])
plt.xlabel('Dá»± Ä‘oÃ¡n')
plt.ylabel('Thá»±c táº¿')
plt.title('Ma tráº­n nháº§m láº«n')
plt.tight_layout()
plt.savefig('xlm_roberta_confusion_matrix.png')
plt.show()

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
f1_micro = f1_score(y_true, y_pred, average='micro')
precision = precision_score(y_true, y_pred, average='micro')
recall = recall_score(y_true, y_pred, average='micro')

print(f"Accuracy: {acc:.4f}")
print(f"F1 - micro: {f1_micro:.4f}")
print(f"Precision (micro): {precision:.4f}")
print(f"Recall (micro): {recall:.4f}")

# Per-class metrics
class_names = ["Negative", "Neutral", "Positive"]
class_precision = precision_score(y_true, y_pred, average=None)
class_recall = recall_score(y_true, y_pred, average=None)
class_f1 = f1_score(y_true, y_pred, average=None)

# Plot class-wise metrics
plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Äá»™ chÃ­nh xÃ¡c')
plt.bar(x, class_recall, width, label='Äá»™ bao phá»§')
plt.bar(x + width, class_f1, width, label='Äiá»ƒm F1')

plt.xlabel('CÃ¡c lá»›p')
plt.ylabel('Äiá»ƒm sá»‘')
plt.title('ÄÃ¡nh giÃ¡ chi tiáº¿t theo lá»›p')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('xlm_roberta_class_metrics.png')
plt.show()

# Summary table
summary_df = pd.DataFrame({
    'Metric': ['Accuracy', 'F1 (micro)', 'Precision (micro)', 'Recall (micro)'],
    'Score': [acc, f1_micro, precision, recall]
})

model_name = "XLM-RoBerta"
metrics = {
    "accuracy": acc,
    "f1_micro": f1_micro,
    "precision": precision,
    "recall": recall
}
all_results[model_name] = metrics

plt.figure(figsize=(8, 3))
plt.axis('off')
table = plt.table(cellText=summary_df.values,
                  colLabels=summary_df.columns,
                  cellLoc='center',
                  loc='center',
                  colWidths=[0.5, 0.3])
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.5)
plt.title('Báº£ng tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡')
plt.tight_layout()
plt.savefig('xlm_roberta_metrics_summary.png')
plt.show()

# Epoch loss plot
plt.figure(figsize=(10, 6))
if loss_tracker.epochs and loss_tracker.eval_losses:
    plt.plot(loss_tracker.epochs, loss_tracker.eval_losses, 'r-o', label='Validation Loss')
    plt.plot(loss_tracker.epochs, loss_tracker.test_losses, 'g-o', label='Test Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Validation and Test Loss by Epoch')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig('xlm_roberta_epoch_loss.png')
    plt.show()

"""# SVM"""

# Support Vector Machine (SVM) Implementation with Visualizations

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from google.colab import drive


# Preprocessing - assuming pre_process_features function is defined elsewhere
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

# Check label distribution
print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# Create a pipeline with TfidfVectorizer and SVM
print("Creating SVM pipeline...")
svm_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('svm', SVC(kernel='rbf', probability=True, random_state=42))
])

# Hyperparameter tuning with GridSearchCV
param_grid = {
    'svm__C': [0.1, 1, 10],
    'svm__gamma': ['scale', 'auto', 0.1, 0.01]
}

print("Performing hyperparameter tuning...")
grid_search = GridSearchCV(
    svm_pipeline,
    param_grid,
    cv=3,
    scoring='f1_micro',  # Changed to micro F1 score
    verbose=1,
    n_jobs=-1
)

# Fit the grid search to find the best parameters
grid_search.fit(train_X, train_y)

# Get the best parameters and model
best_params = grid_search.best_params_
best_svm = grid_search.best_estimator_
print(f"Best parameters: {best_params}")

# Train with the optimal parameters
print("Training final SVM model with optimal parameters...")
best_svm.fit(train_X, train_y)

# Make predictions on validation and test sets
print("Making predictions...")
dev_pred = best_svm.predict(dev_X)
test_pred = best_svm.predict(test_X)

# Calculate probabilities for ROC curve
test_probs = best_svm.predict_proba(test_X)

# Get training, validation, and test accuracy
train_acc = best_svm.score(train_X, train_y)
dev_acc = accuracy_score(dev_y, dev_pred)
test_acc = accuracy_score(test_y, test_pred)

# -----------------------------
# Calculate "loss" as (1 - accuracy)
# -----------------------------
train_loss = 1 - train_acc
dev_loss = 1 - dev_acc
test_loss = 1 - test_acc

# -----------------------------
# Plotting Losses - Line + Bar Plot
# -----------------------------
losses = [train_loss, dev_loss, test_loss]
datasets = ['Train', 'Validation', 'Test']

fig, ax1 = plt.subplots(figsize=(8, 6))

# Bar plot
color = 'tab:blue'
ax1.bar(datasets, losses, color=color, alpha=0.6, label='Loss (1 - Accuracy)')
ax1.set_ylabel('Loss', color=color)
ax1.set_ylim(0, 1)
ax1.set_title('Loss on Train, Validation, and Test Sets')
ax1.legend(loc='upper left')

# Line plot
ax2 = ax1.twinx()
color = 'tab:red'
ax2.plot(datasets, losses, color=color, marker='o', linewidth=2, label='Loss Trend')
ax2.set_ylabel('Loss', color=color)
ax2.set_ylim(0, 1)
ax2.legend(loc='upper right')

fig.tight_layout()
plt.savefig('/content/drive/MyDrive/NCKH_2425/svm_losses.png')
plt.show()

print(f"Training accuracy: {train_acc:.4f}")
print(f"Validation accuracy: {dev_acc:.4f}")
print(f"Test accuracy: {test_acc:.4f}")

# Calculate metrics for test set (only micro metrics)
f1_micro = f1_score(test_y, test_pred, average='micro')
precision_micro = precision_score(test_y, test_pred, average='micro')
recall_micro = recall_score(test_y, test_pred, average='micro')

print(f"F1 - micro: {f1_micro:.4f}")
print(f"Precision (micro): {precision_micro:.4f}")
print(f"Recall (micro): {recall_micro:.4f}")

model_name = "SVM"

metrics = {
    "accuracy": test_acc,
    "f1_micro": f1_micro,
    "precision_micro": precision_micro,
    "recall_micro": recall_micro
}

all_results[model_name] = metrics

# Confusion Matrix
cf_matrix = confusion_matrix(test_y, test_pred)
plt.figure(figsize=(6, 4))
sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["negative", "neutral", "positive"],
           yticklabels=["negative", "neutral", "positive"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/NCKH_2425/confusion_matrix.png')
plt.show()

# Class-wise metrics
class_names = ["negative", "neutral", "positive"]
class_precision = precision_score(test_y, test_pred, average=None)
class_recall = recall_score(test_y, test_pred, average=None)
class_f1 = f1_score(test_y, test_pred, average=None)

plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Precision')
plt.bar(x, class_recall, width, label='Recall')
plt.bar(x + width, class_f1, width, label='F1-score')

plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Class-wise Performance Metrics')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('/content/drive/MyDrive/NCKH_2425/svm_class_metrics.png')
plt.show()

# Print some example predictions
print("\nExample predictions:")
print("------------------------")
for i in range(min(10, len(test_X))):
    print(f"Text: {test_X[i][:50]}...")
    print(f"True sentiment: {test_y[i]}")
    print(f"Predicted sentiment: {test_pred[i]}")
    print("------------------------")

# # Save model to Google Drive (if needed)
# from sklearn.externals import joblib
# save_path = '/content/drive/MyDrive/NCKH_2425/svm_model.pkl'
# joblib.dump(best_svm, save_path)
# print(f"SVM model saved to Google Drive at: {save_path}")

"""# LSTM"""

# LSTM Implementation with Visualizations
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard
import os
import time
import datetime


# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Preprocessing - assuming pre_process_features function is defined elsewhere
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

# Check label distribution
print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# Text vectorization with Keras Tokenizer
max_words = 30000  # Maximum vocab size
max_len = 150      # Maximum sequence length

print("Tokenizing text data...")
tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_X)

# Convert text to sequences
train_sequences = tokenizer.texts_to_sequences(train_X)
dev_sequences = tokenizer.texts_to_sequences(dev_X)
test_sequences = tokenizer.texts_to_sequences(test_X)

# Pad sequences to the same length
train_padded = pad_sequences(train_sequences, maxlen=max_len, padding='post')
dev_padded = pad_sequences(dev_sequences, maxlen=max_len, padding='post')
test_padded = pad_sequences(test_sequences, maxlen=max_len, padding='post')

# Vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary size: {vocab_size}")

# Build LSTM model
print("Building LSTM model...")
model = Sequential([
    Embedding(vocab_size, 128, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Bidirectional(LSTM(32)),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')  # 3 classes
])

# Compile model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Model summary
model.summary()

# Setup callbacks
checkpoint_path = "/content/drive/MyDrive/NCKH_2425/LSTM/best_model.h5"
log_dir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

callbacks = [
    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),
    ModelCheckpoint(
        filepath=checkpoint_path,
        save_best_only=True,
        monitor='val_loss',
        mode='min',
        verbose=1
    ),
    TensorBoard(log_dir=log_dir, histogram_freq=1)
]

# Train model
print("Training LSTM model...")
batch_size = 32
epochs = 10

history = model.fit(
    train_padded, train_y,
    validation_data=(dev_padded, dev_y),
    epochs=epochs,
    batch_size=batch_size,
    callbacks=callbacks,
    verbose=1
)

# Save the model
model.save("/content/drive/MyDrive/NCKH_2425/LSTM/lstm_final_model.h5")
print("LSTM model saved to disk.")

# Evaluate on test set
print("Evaluating on test set...")
test_loss, test_acc = model.evaluate(test_padded, test_y)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_acc:.4f}")

# Make predictions
y_pred_prob = model.predict(test_padded)
y_pred = np.argmax(y_pred_prob, axis=1)

# Calculate metrics
f1_micro = f1_score(test_y, y_pred, average='micro')
precision = precision_score(test_y, y_pred, average='micro')
recall = recall_score(test_y, y_pred, average='micro')

print(f"F1 - micro: {f1_micro:.4f}")
print(f"Precision (micro): {precision:.4f}")
print(f"Recall (micro): {recall:.4f}")

# Plot training history
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.savefig('lstm_training_history.png')
plt.show()

# Create confusion matrix
class_names = ["Negative", "Neutral", "Positive"]
cf_matrix = confusion_matrix(test_y, y_pred)
plt.figure(figsize=(6, 4))  # Smaller size
sns.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=class_names,
           yticklabels=class_names)
plt.xlabel('Dá»± Ä‘oÃ¡n')
plt.ylabel('Thá»±c táº¿')
plt.title('Ma tráº­n nháº§m láº«n - LSTM')
plt.tight_layout()
plt.savefig('lstm_confusion_matrix.png')
plt.show()

# Calculate class-wise metrics
class_precision = precision_score(test_y, y_pred, average=None)
class_recall = recall_score(test_y, y_pred, average=None)
class_f1 = f1_score(test_y, y_pred, average=None)

# Plot class-wise metrics
plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Äá»™ chÃ­nh xÃ¡c')
plt.bar(x, class_recall, width, label='Äá»™ bao phá»§')
plt.bar(x + width, class_f1, width, label='Äiá»ƒm F1')

plt.xlabel('CÃ¡c lá»›p')
plt.ylabel('Äiá»ƒm sá»‘')
plt.title('ÄÃ¡nh giÃ¡ chi tiáº¿t theo lá»›p - LSTM')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('lstm_class_metrics.png')
plt.show()

# Create a summary table of all metrics
summary_df = pd.DataFrame({
    'Metric': ['Accuracy', 'F1 (micro)', 'Precision (micro)', 'Recall (micro)'],
    'Score': [test_acc, f1_micro, precision, recall]
})

# Plot as a table
plt.figure(figsize=(8, 3))
plt.axis('off')
table = plt.table(cellText=summary_df.values,
                 colLabels=summary_df.columns,
                 cellLoc='center',
                 loc='center',
                 colWidths=[0.5, 0.3])
table.auto_set_font_size(False)
table.set_fontsize(12)
table.scale(1.2, 1.5)
plt.title('Báº£ng tÃ³m táº¯t káº¿t quáº£ Ä‘Ã¡nh giÃ¡ - LSTM')
plt.tight_layout()
plt.savefig('lstm_metrics_summary.png')
plt.show()

# Plot prediction confidence
plt.figure(figsize=(10, 6))
confidence_scores = np.max(y_pred_prob, axis=1)
plt.hist(confidence_scores, bins=20, alpha=0.7)
plt.axvline(x=0.5, color='red', linestyle='--', label='50% Confidence')
plt.axvline(x=0.75, color='green', linestyle='--', label='75% Confidence')
plt.title('Distribution of Prediction Confidence')
plt.xlabel('Confidence Score')
plt.ylabel('Number of Predictions')
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('lstm_confidence_distribution.png')
plt.show()

model_name = "LSTM"
metrics = {
    "accuracy": test_acc,
    "f1_micro": f1_micro,
    "precision": precision,
    "recall": recall
}
all_results[model_name] = metrics

# Identify most uncertain predictions
uncertain_indices = np.argsort(confidence_scores)[:20]  # 20 most uncertain predictions
for idx in uncertain_indices:
    print(f"Text: {test_X[idx]}")
    print(f"True label: {class_names[test_y[idx]]}")
    print(f"Predicted: {class_names[y_pred[idx]]} with confidence {confidence_scores[idx]:.4f}")
    print("-" * 50)

"""# Compare"""

import { useState } from 'react';
import {
  BarChart,
  Bar,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  Legend,
  ResponsiveContainer,
  RadarChart,
  PolarGrid,
  PolarAngleAxis,
  PolarRadiusAxis,
  Radar
} from 'recharts';

export default function ModelComparisonCharts() {
  // Giáº£ Ä‘á»‹nh dá»¯ liá»‡u tá»« all_results
  #const all_results = {
  #   "ViBert4News": {
  #     "accuracy": 0.89,
  #     "f1_micro": 0.88,
  #     "precision": 0.87,
  #     "recall": 0.88
  #   },
  #   "PhoBERT": {
  #     "accuracy": 0.92,
  #     "f1_micro": 0.91,
  #     "precision": 0.90,
  #     "recall": 0.92
  #   },
  #   "mBERT": {
  #     "accuracy": 0.85,
  #     "f1_micro": 0.84,
  #     "precision": 0.83,
  #     "recall": 0.85
  #   },
  #   "XLM-R": {
  #     "accuracy": 0.91,
  #     "f1_micro": 0.90,
  #     "precision": 0.89,
  #     "recall": 0.90
  #   },
  #   "ViT5": {
  #     "accuracy": 0.88,
  #     "f1_micro": 0.87,
  #     "precision": 0.86,
  #     "recall": 0.88
  #   },
  #   "BARTpho": {
  #     "accuracy": 0.90,
  #     "f1_micro": 0.89,
  #     "precision": 0.88,
  #     "recall": 0.90
  #   }
  # };

  // Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u cho biá»ƒu Ä‘á»“ cá»™t
  const barChartData = Object.keys(all_results).map(model => {
    return {
      model,
      ...all_results[model]
    };
  });

  // Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u cho biá»ƒu Ä‘á»“ radar
  const radarChartData = [
    { subject: 'Accuracy', ...Object.keys(all_results).reduce((acc, model) => {
      acc[model] = all_results[model].accuracy;
      return acc;
    }, {})},
    { subject: 'F1 Micro', ...Object.keys(all_results).reduce((acc, model) => {
      acc[model] = all_results[model].f1_micro;
      return acc;
    }, {})},
    { subject: 'Precision', ...Object.keys(all_results).reduce((acc, model) => {
      acc[model] = all_results[model].precision;
      return acc;
    }, {})},
    { subject: 'Recall', ...Object.keys(all_results).reduce((acc, model) => {
      acc[model] = all_results[model].recall;
      return acc;
    }, {})}
  ];

  // Danh sÃ¡ch cÃ¡c mÃ´ hÃ¬nh vÃ  colors cho biá»ƒu Ä‘á»“
  const models = Object.keys(all_results);
  const colors = ['#8884d8', '#82ca9d', '#ffc658', '#ff8042', '#0088fe', '#00C49F'];

  const [selectedModel, setSelectedModel] = useState(models[0]);

  return (
    <div className="flex flex-col space-y-8 p-4">
      <h1 className="text-2xl font-bold text-center">So SÃ¡nh Hiá»‡u Suáº¥t CÃ¡c MÃ´ HÃ¬nh NLP</h1>

      {/* Biá»ƒu Ä‘á»“ cá»™t so sÃ¡nh táº¥t cáº£ metrics cá»§a táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh */}
      <div className="space-y-2">
        <h2 className="text-xl font-semibold">So SÃ¡nh Táº¥t Cáº£ Metrics</h2>
        <div className="h-96">
          <ResponsiveContainer width="100%" height="100%">
            <BarChart
              data={barChartData}
              margin={{ top: 20, right: 30, left: 20, bottom: 70 }}
            >
              <CartesianGrid strokeDasharray="3 3" />
              <XAxis dataKey="model" angle={-45} textAnchor="end" interval={0} height={70} />
              <YAxis domain={[0, 1]} />
              <Tooltip formatter={(value) => (value * 100).toFixed(1) + '%'} />
              <Legend />
              <Bar dataKey="accuracy" name="Accuracy" fill="#8884d8" />
              <Bar dataKey="f1_micro" name="F1 Micro" fill="#82ca9d" />
              <Bar dataKey="precision" name="Precision" fill="#ffc658" />
              <Bar dataKey="recall" name="Recall" fill="#ff8042" />
            </BarChart>
          </ResponsiveContainer>
        </div>
      </div>

      {/* Biá»ƒu Ä‘á»“ radar cho tá»«ng mÃ´ hÃ¬nh */}
      <div className="space-y-2">
        <div className="flex items-center justify-between">
          <h2 className="text-xl font-semibold">PhÃ¢n TÃ­ch Hiá»‡u Suáº¥t Theo MÃ´ HÃ¬nh</h2>
          <div className="flex items-center space-x-2">
            <label className="text-sm font-medium">Chá»n MÃ´ HÃ¬nh:</label>
            <select
              className="border rounded p-1"
              value={selectedModel}
              onChange={(e) => setSelectedModel(e.target.value)}
            >
              {models.map(model => (
                <option key={model} value={model}>{model}</option>
              ))}
            </select>
          </div>
        </div>

        <div className="flex flex-wrap">
          {/* Biá»ƒu Ä‘á»“ Radar */}
          <div className="w-full md:w-1/2 h-80">
            <ResponsiveContainer width="100%" height="100%">
              <RadarChart outerRadius={90} data={radarChartData}>
                <PolarGrid />
                <PolarAngleAxis dataKey="subject" />
                <PolarRadiusAxis angle={30} domain={[0, 1]} />
                {models.map((model, index) => (
                  <Radar
                    key={model}
                    name={model}
                    dataKey={model}
                    stroke={colors[index % colors.length]}
                    fill={colors[index % colors.length]}
                    fillOpacity={model === selectedModel ? 0.6 : 0.1}
                    strokeWidth={model === selectedModel ? 2 : 1}
                  />
                ))}
                <Legend />
                <Tooltip formatter={(value) => (value * 100).toFixed(1) + '%'} />
              </RadarChart>
            </ResponsiveContainer>
          </div>

          {/* Báº£ng so sÃ¡nh chi tiáº¿t */}
          <div className="w-full md:w-1/2 p-4">
            <h3 className="text-lg font-medium mb-2">Chi Tiáº¿t Metrics: {selectedModel}</h3>
            <div className="border rounded overflow-hidden">
              <table className="min-w-full divide-y divide-gray-200">
                <thead className="bg-gray-100">
                  <tr>
                    <th className="p-2 text-left text-sm font-medium">Metric</th>
                    <th className="p-2 text-left text-sm font-medium">GiÃ¡ Trá»‹</th>
                  </tr>
                </thead>
                <tbody className="bg-white divide-y divide-gray-200">
                  {Object.entries(all_results[selectedModel]).map(([metric, value]) => (
                    <tr key={metric}>
                      <td className="p-2 text-sm">{metric.charAt(0).toUpperCase() + metric.slice(1)}</td>
                      <td className="p-2 text-sm">{(value * 100).toFixed(2)}%</td>
                    </tr>
                  ))}
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>

      {/* Biá»ƒu Ä‘á»“ so sÃ¡nh tá»«ng metric giá»¯a cÃ¡c mÃ´ hÃ¬nh */}
      <div className="space-y-4">
        <h2 className="text-xl font-semibold">So SÃ¡nh Tá»«ng Metric</h2>
        <div className="grid grid-cols-1 md:grid-cols-2 gap-6">
          {['accuracy', 'f1_micro', 'precision', 'recall'].map(metric => (
            <div key={metric} className="h-64">
              <h3 className="text-lg font-medium mb-1 capitalize">{metric.replace('_', ' ')}</h3>
              <ResponsiveContainer width="100%" height="90%">
                <BarChart
                  data={barChartData}
                  margin={{ top: 5, right: 30, left: 20, bottom: 30 }}
                >
                  <CartesianGrid strokeDasharray="3 3" />
                  <XAxis dataKey="model" angle={-45} textAnchor="end" interval={0} height={50} />
                  <YAxis domain={[0, 1]} />
                  <Tooltip formatter={(value) => (value * 100).toFixed(1) + '%'} />
                  <Bar
                    dataKey={metric}
                    fill="#8884d8"
                    name={metric.charAt(0).toUpperCase() + metric.slice(1).replace('_', ' ')}
                  />
                </BarChart>
              </ResponsiveContainer>
            </div>
          ))}
        </div>
      </div>
    </div>
  );
}

"""# DEMO"""

!pip install transformers underthesea

import torch
import re
import numpy as np
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from underthesea import ner, word_tokenize, pos_tag
from sklearn.preprocessing import LabelEncoder

# Properly configure device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# HÃ m tiá»n xá»­ lÃ½ vÄƒn báº£n giá»¯ nguyÃªn
def load_stopwords(file_path):
    """Äá»c danh sÃ¡ch stopwords tá»« file"""
    with open(file_path, "r", encoding="utf-8") as ins:
        stopwords = [line.strip('\n') for line in ins]
    return set(stopwords)

def filter_stop_words(text, stop_words):
    """Loáº¡i bá» stopwords khá»i vÄƒn báº£n"""
    new_sent = [word for word in text.split() if word not in stop_words]
    return ' '.join(new_sent)

def deEmojify(text):
    """Loáº¡i bá» emoji khá»i vÄƒn báº£n"""
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

def preprocess_text(text, tokenizer=None, remove_stopwords=False, stopwords=None,
                   remove_emoji=True, lowercase=True):
    """Tiá»n xá»­ lÃ½ vÄƒn báº£n tá»•ng há»£p"""
    if remove_emoji:
        text = deEmojify(text)

    # Tokenize náº¿u cÃ³ tokenizer
    if tokenizer:
        sentences = tokenizer.tokenize(text)
        text = " ".join([" ".join(sentence) for sentence in sentences])

    # Loáº¡i bá» stopwords náº¿u cáº§n
    if remove_stopwords and stopwords:
        text = filter_stop_words(text, stopwords)

    # Chuyá»ƒn vá» chá»¯ thÆ°á»ng náº¿u cáº§n
    if lowercase:
        text = text.lower()

    return text

# Táº£i vÃ  chuáº©n bá»‹ mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc
def load_sentiment_model(model_path, tokenizer_name="vinai/phobert-base-v2"):
    """Táº£i mÃ´ hÃ¬nh phÃ¢n loáº¡i cáº£m xÃºc"""
    # Sá»­a lá»—i: Äáº£m báº£o model Ä‘Æ°á»£c táº£i Ä‘Ãºng cÃ¡ch trÆ°á»›c khi Ä‘Æ°a vÃ o pipeline
    try:
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)

        # Move model to device after loading
        model = model.to(device)

        # Create pipeline after moving model to device
        device_id = 0 if device.type == "cuda" else -1
        sentiment_classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=device_id)

        return sentiment_classifier, tokenizer
    except Exception as e:
        print(f"Error loading model: {e}")
        # Fallback to CPU if there's an issue with GPU
        print("Falling back to CPU")
        tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=False)
        model = AutoModelForSequenceClassification.from_pretrained(model_path)
        sentiment_classifier = pipeline("text-classification", model=model, tokenizer=tokenizer, device=-1)
        return sentiment_classifier, tokenizer

# PHÆ¯Æ NG PHÃP Cáº¢I TIáº¾N 1: PhÃ¢n tÃ­ch tá»«ng tá»« vá»›i ngá»¯ cáº£nh linh hoáº¡t
def analyze_words_sentiment_adaptive(text, sentiment_classifier, min_window=3, max_window=7):
    """PhÃ¢n tÃ­ch cáº£m xÃºc cá»§a tá»«ng tá»« vá»›i kÃ­ch thÆ°á»›c cá»­a sá»• ngá»¯ cáº£nh linh hoáº¡t"""
    words = word_tokenize(text, format="text").split()
    pos_tags = pos_tag(text)  # Tráº£ vá» dáº¡ng [(word, tag)]

    # Lá»c chá»‰ láº¥y danh tá»« (N), Ä‘á»™ng tá»« (V), tÃ­nh tá»« (A), phÃ³ tá»« (R)
    filtered_words_with_pos = [(word, tag) for word, tag in pos_tags if tag.startswith(("N", "V", "A", "R"))]
    filtered_words = [word for word, _ in filtered_words_with_pos]

    # Tá»« Ä‘iá»ƒn lÆ°u cáº£m xÃºc cá»§a tá»«ng tá»« vÃ  Ä‘á»™ tin cáº­y
    word_sentiments = {}

    for i, word in enumerate(words):
        if word not in filtered_words:
            continue

        # Láº¥y POS tag cá»§a tá»«
        word_tag = next((tag for w, tag in filtered_words_with_pos if w == word), None)

        # Äiá»u chá»‰nh kÃ­ch thÆ°á»›c cá»­a sá»• dá»±a vÃ o loáº¡i tá»«
        if word_tag and word_tag.startswith('A'):  # TÃ­nh tá»« - window nhá» hÆ¡n
            window_size = min_window
        elif word_tag and word_tag.startswith('N'):  # Danh tá»« - window trung bÃ¬nh
            window_size = (min_window + max_window) // 2
        else:  # Äá»™ng tá»«, phÃ³ tá»« - window lá»›n hÆ¡n
            window_size = max_window

        start = max(0, i - window_size // 2)
        end = min(len(words), i + window_size // 2 + 1)
        phrase = " ".join(words[start:end])

        try:
            result = sentiment_classifier(phrase)
            label = result[0]["label"]
            score = result[0]["score"]
            sentiment = {"LABEL_0": "Negative", "LABEL_1": "Neutral", "LABEL_2": "Positive"}[label]

            # LÆ°u cáº£ cáº£m xÃºc vÃ  Ä‘á»™ tin cáº­y
            word_sentiments[word] = {
                "sentiment": sentiment,
                "confidence": score,
                "context": phrase
            }
        except Exception as e:
            print(f"âš ï¸ Bá» qua cá»¥m '{phrase}' do lá»—i: {e}")
            continue

    return word_sentiments

# PHÆ¯Æ NG PHÃP Cáº¢I TIáº¾N 2: PhÃ¢n tÃ­ch theo cá»¥m tá»« cÃ³ Ã½ nghÄ©a
def analyze_phrases_sentiment(text, sentiment_classifier):
    """PhÃ¢n tÃ­ch cáº£m xÃºc theo cá»¥m tá»« cÃ³ Ã½ nghÄ©a"""
    # Sá»­ dá»¥ng NER Ä‘á»ƒ tÃ¬m cÃ¡c thá»±c thá»ƒ cÃ³ tÃªn
    entities = ner(text)

    # PhÃ¢n tÃ­ch tá»« loáº¡i Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cá»¥m danh tá»« tiá»m nÄƒng
    pos_tags = pos_tag(text)

    # TrÃ­ch xuáº¥t cÃ¡c cá»¥m danh tá»« (N + A), Ä‘á»™ng tá»« + danh tá»« (V + N), v.v.
    phrases = []
    i = 0
    while i < len(pos_tags) - 1:
        word, tag = pos_tags[i]

        # TÃ¬m cá»¥m "danh tá»« + tÃ­nh tá»«" hoáº·c "tÃ­nh tá»« + danh tá»«"
        if (tag.startswith('N') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('A')) or \
           (tag.startswith('A') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('N')):
            phrases.append(" ".join([pos_tags[i][0], pos_tags[i+1][0]]))
            i += 2

        # TÃ¬m cá»¥m "Ä‘á»™ng tá»« + danh tá»«"
        elif tag.startswith('V') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('N'):
            phrases.append(" ".join([pos_tags[i][0], pos_tags[i+1][0]]))
            i += 2

        # TÃ¬m cá»¥m "Ä‘á»™ng tá»« + phÃ³ tá»«" hoáº·c "phÃ³ tá»« + Ä‘á»™ng tá»«"
        elif (tag.startswith('V') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('R')) or \
             (tag.startswith('R') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('V')):
            phrases.append(" ".join([pos_tags[i][0], pos_tags[i+1][0]]))
            i += 2

        # TÃ¬m cá»¥m "phÃ³ tá»« + tÃ­nh tá»«"
        elif tag.startswith('R') and i+1 < len(pos_tags) and pos_tags[i+1][1].startswith('A'):
            phrases.append(" ".join([pos_tags[i][0], pos_tags[i+1][0]]))
            i += 2

        else:
            # Danh tá»«, Ä‘á»™ng tá»«, tÃ­nh tá»« Ä‘Æ¡n láº» cÃ³ Ã½ nghÄ©a
            if tag.startswith(('N', 'V', 'A')) and len(word) > 1:  # TrÃ¡nh cÃ¡c tá»« Ä‘Æ¡n
                phrases.append(word)
            i += 1

    # ThÃªm cÃ¡c thá»±c thá»ƒ Ä‘Ã£ xÃ¡c Ä‘á»‹nh tá»« NER
    for entity in entities:
        if isinstance(entity, tuple) and len(entity) >= 3:  # Dáº¡ng [(word, tag, probability), ...]
            entity_text = entity[0]
            if len(entity_text.split()) > 1:  # Chá»‰ láº¥y cÃ¡c thá»±c thá»ƒ multi-word
                phrases.append(entity_text)

    # Loáº¡i bá» trÃ¹ng láº·p
    phrases = list(set(phrases))

    # PhÃ¢n tÃ­ch cáº£m xÃºc cho tá»«ng cá»¥m
    phrase_sentiments = {}
    for phrase in phrases:
        try:
            result = sentiment_classifier(phrase)
            label = result[0]["label"]
            score = result[0]["score"]
            sentiment = {"LABEL_0": "Negative", "LABEL_1": "Neutral", "LABEL_2": "Positive"}[label]

            # Chá»‰ lÆ°u cÃ¡c cá»¥m cÃ³ cáº£m xÃºc rÃµ rÃ ng (Ä‘á»™ tin cáº­y cao)
            if score > 0.7 and sentiment != "Neutral":
                phrase_sentiments[phrase] = {
                    "sentiment": sentiment,
                    "confidence": score
                }
        except Exception as e:
            print(f"âš ï¸ Bá» qua cá»¥m '{phrase}' do lá»—i: {e}")
            continue

    return phrase_sentiments

# PHÆ¯Æ NG PHÃP Cáº¢I TIáº¾N 3: Káº¿t há»£p phÃ¢n tÃ­ch vÃ  lá»c káº¿t quáº£ cháº¥t lÆ°á»£ng
def extract_sentiment_words(text, sentiment_classifier, threshold=0.75):
    """Káº¿t há»£p cÃ¡c phÆ°Æ¡ng phÃ¡p vÃ  lá»c káº¿t quáº£ cháº¥t lÆ°á»£ng cao"""
    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
    processed_text = preprocess_text(text, remove_emoji=True, lowercase=True)

    # PhÃ¢n tÃ­ch theo tá»« vá»›i ngá»¯ cáº£nh linh hoáº¡t
    word_sentiments = analyze_words_sentiment_adaptive(processed_text, sentiment_classifier)

    # PhÃ¢n tÃ­ch theo cá»¥m tá»« cÃ³ Ã½ nghÄ©a
    phrase_sentiments = analyze_phrases_sentiment(processed_text, sentiment_classifier)

    # Káº¿t há»£p vÃ  lá»c káº¿t quáº£
    all_sentiment_items = {}

    # ThÃªm cáº£m xÃºc tá»« word_sentiments cÃ³ Ä‘á»™ tin cáº­y cao
    for word, data in word_sentiments.items():
        if data["confidence"] >= threshold:
            all_sentiment_items[word] = data

    # ThÃªm cáº£m xÃºc tá»« phrase_sentiments
    for phrase, data in phrase_sentiments.items():
        if data["confidence"] >= threshold:
            all_sentiment_items[phrase] = data

    # PhÃ¢n loáº¡i theo cáº£m xÃºc
    positive_items = {k: v for k, v in all_sentiment_items.items() if v["sentiment"] == "Positive"}
    negative_items = {k: v for k, v in all_sentiment_items.items() if v["sentiment"] == "Negative"}
    neutral_items = {k: v for k, v in all_sentiment_items.items() if v["sentiment"] == "Neutral"}

    # Sáº¯p xáº¿p theo Ä‘á»™ tin cáº­y (tá»« cao Ä‘áº¿n tháº¥p)
    positive_items = dict(sorted(positive_items.items(), key=lambda x: x[1]["confidence"], reverse=True))
    negative_items = dict(sorted(negative_items.items(), key=lambda x: x[1]["confidence"], reverse=True))
    neutral_items = dict(sorted(neutral_items.items(), key=lambda x: x[1]["confidence"], reverse=True))

    return {
        "positive": positive_items,
        "negative": negative_items,
        "neutral": neutral_items
    }

# PhÃ¢n tÃ­ch cáº£m xÃºc toÃ n cÃ¢u
def analyze_full_sentence_sentiment(text, sentiment_classifier):
    """PhÃ¢n tÃ­ch cáº£m xÃºc cá»§a toÃ n bá»™ cÃ¢u"""
    try:
        results = sentiment_classifier(text)
        predicted_label = results[0]["label"]
        predicted_score = results[0]["score"]
        attitude_map = {"LABEL_0": "Negative", "LABEL_1": "Neutral", "LABEL_2": "Positive"}
        return attitude_map.get(predicted_label, "Unknown"), predicted_score
    except Exception as e:
        print(f"Lá»—i khi phÃ¢n tÃ­ch cáº£m xÃºc toÃ n cÃ¢u: {e}")
        return "Unknown", 0.0

# HÃ m chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng
def analyze_sentiment_words(text, model_path, stopwords_path=None):
    """HÃ m chÃ­nh Ä‘á»ƒ phÃ¢n tÃ­ch tá»« tÃ¬nh cáº£m trong vÄƒn báº£n"""
    # Táº£i mÃ´ hÃ¬nh cáº£m xÃºc
    print("Äang táº£i mÃ´ hÃ¬nh sentiment...")
    sentiment_classifier, tokenizer = load_sentiment_model(model_path)
    print("ÄÃ£ táº£i xong mÃ´ hÃ¬nh sentiment")

    # Táº£i stopwords náº¿u cÃ³
    stopwords = None
    if stopwords_path:
        try:
            stopwords = load_stopwords(stopwords_path)
            print(f"ÄÃ£ táº£i {len(stopwords)} stopwords")
        except Exception as e:
            print(f"KhÃ´ng thá»ƒ táº£i stopwords: {e}")
            print("Tiáº¿p tá»¥c mÃ  khÃ´ng cÃ³ stopwords")

    # Tiá»n xá»­ lÃ½ vÄƒn báº£n
    processed_text = preprocess_text(text, remove_emoji=True, lowercase=True,
                                   remove_stopwords=bool(stopwords), stopwords=stopwords)

    # PhÃ¢n tÃ­ch cáº£m xÃºc toÃ n vÄƒn
    print("\nğŸ” Äang phÃ¢n tÃ­ch cáº£m xÃºc toÃ n vÄƒn...")
    attitude, score = analyze_full_sentence_sentiment(processed_text, sentiment_classifier)

    # TrÃ­ch xuáº¥t tá»« tÃ¬nh cáº£m
    print("\nğŸ” Äang trÃ­ch xuáº¥t tá»« tÃ¬nh cáº£m...")
    sentiment_items = extract_sentiment_words(processed_text, sentiment_classifier)

    # In káº¿t quáº£
    print(f"\nâœ… Káº¿t quáº£ phÃ¢n tÃ­ch cáº£m xÃºc:")
    print(f"âœ… Cáº£m xÃºc tá»•ng thá»ƒ: {attitude} (Ä‘iá»ƒm: {score:.2f})")

    # Hiá»ƒn thá»‹ cÃ¡c tá»« tÃ­ch cá»±c
    positive_words = list(sentiment_items["positive"].keys())
    print(f"âœ… Tá»« tÃ­ch cá»±c ({len(positive_words)}): {', '.join(positive_words[:15])}{'...' if len(positive_words) > 15 else ''}")

    # Hiá»ƒn thá»‹ cÃ¡c tá»« tiÃªu cá»±c
    negative_words = list(sentiment_items["negative"].keys())
    print(f"âœ… Tá»« tiÃªu cá»±c ({len(negative_words)}): {', '.join(negative_words[:15])}{'...' if len(negative_words) > 15 else ''}")

    # Hiá»ƒn thá»‹ cÃ¡c tá»« trung tÃ­nh
    neutral_words = list(sentiment_items["neutral"].keys())
    print(f"âœ… Tá»« trung tÃ­nh ({len(neutral_words)}): {', '.join(neutral_words[:15])}{'...' if len(neutral_words) > 15 else ''}")

    # Tráº£ vá» káº¿t quáº£ Ä‘á»ƒ sá»­ dá»¥ng trong cÃ¡c hÃ m khÃ¡c
    return {
        "overall_sentiment": attitude,
        "overall_score": score,
        "sentiment_items": sentiment_items
    }

# VÃ­ dá»¥ sá»­ dá»¥ng
if __name__ == "__main__":
    # Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n
    model_path = "/content/drive/MyDrive/NCKH_2425/ViBert4News/Base"
    model_path_2 = "/content/drive/MyDrive/NCKH_2425/PhoBERT/PhoBERT_base_v2"
    model_path_3 = "/content/drive/MyDrive/NCKH_2425/XLM-RoBERTa"
    stopwords_path = "/content/drive/MyDrive/NCKH_2425/DATASET/#dataVNstopwords/vietnamese-stopwords-dash.txt"

    # VÄƒn báº£n vÃ­ dá»¥
    example_text = """Há»i tháº­t lÃ  Ä‘i cá»• vÅ© ÄÆ°á»ng Äáº¿n ThÃ nh CÃ´ng cÃ³ Ä‘Æ°á»£c cá»™ng Ä‘iá»ƒm rÃ¨n luyá»‡n k váº­y áº¡. Chá»© nÄƒm nÃ o mÃ¬nh cÅ©ng Ä‘i cÃ³ tháº¥y Ä‘Æ°á»£c cá»™ng má»¥c nÃ o Ä‘Ã¢u"""

    # PhÃ¢n tÃ­ch
    results = analyze_sentiment_words(example_text, model_path, stopwords_path)
    results_2 = analyze_sentiment_words(example_text, model_path_2, stopwords_path)
    results_3 = analyze_sentiment_words(example_text, model_path_3, stopwords_path)

# Bert4news - Training on Colab

# 1. Import libraries
import os
import numpy as np
import torch
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sn
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score, recall_score, precision_score
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments
from transformers import EarlyStoppingCallback, TrainerCallback, set_seed

# 2. Setup
# Set environment variables for better logging
os.environ["WANDB_DISABLED"] = "true"
os.environ["WANDB_MODE"] = "offline"

# Set random seed
set_seed(42)

# Label mapping
label_mapping = {0: "negative", 1: "neutral", 2: "positive"}

# Model & output directory (for Colab)
base_model_dir = "/content/drive/MyDrive/NCKH_2425/ViBert4News/Base"

# 3. Preprocess data
train_X, train_y = pre_process_features(X_train, y_train, tokenized=False, lowercased=False)
dev_X, dev_y = pre_process_features(X_dev, y_dev, tokenized=False, lowercased=False)
test_X, test_y = pre_process_features(X_test, y_test, tokenized=False, lowercased=False)

print(f"Label distribution in training set: {np.bincount(train_y)}")
print(f"Label distribution in dev set: {np.bincount(dev_y)}")
print(f"Label distribution in test set: {np.bincount(test_y)}")

# 4. Load model and tokenizer
model = AutoModelForSequenceClassification.from_pretrained(base_model_dir, num_labels=3)
tokenizer = AutoTokenizer.from_pretrained(base_model_dir, use_fast=True)

# 5. Dataset class
class BuildDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# Tokenize data
train_encodings = tokenizer(train_X, truncation=True, padding=True, max_length=512)
dev_encodings = tokenizer(dev_X, truncation=True, padding=True, max_length=512)
test_encodings = tokenizer(test_X, truncation=True, padding=True, max_length=512)

# Create datasets
train_dataset = BuildDataset(train_encodings, train_y)
dev_dataset = BuildDataset(dev_encodings, dev_y)
test_dataset = BuildDataset(test_encodings, test_y)

# 6. Custom Loss Tracking Callback
class LossTrackingCallback(TrainerCallback):
    def __init__(self):
        self.train_losses = []
        self.eval_losses = []
        self.test_losses = []
        self.steps = []
        self.epochs = []
        self.trainer = None
        self.test_dataset = None

    def on_train_begin(self, args, state, control, **kwargs):
        if 'trainer' in kwargs:
            self.trainer = kwargs['trainer']

    def set_trainer_and_dataset(self, trainer, test_dataset):
        self.trainer = trainer
        self.test_dataset = test_dataset

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs is not None:
            if "loss" in logs:
                self.train_losses.append(logs["loss"])
                self.steps.append(state.global_step)
            if "eval_loss" in logs:
                self.eval_losses.append(logs["eval_loss"])
                if self.trainer is not None and self.test_dataset is not None:
                    test_outputs = self.trainer.predict(self.test_dataset)
                    test_loss = test_outputs.metrics["test_loss"]
                    self.test_losses.append(test_loss)
                    self.epochs.append(state.epoch)
                    print(f"Epoch {state.epoch:.2f}: Eval loss: {logs['eval_loss']:.4f}, Test loss: {test_loss:.4f}")

# 7. Training arguments
training_args = TrainingArguments(
    output_dir=base_model_dir,
    num_train_epochs=6,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    learning_rate=3e-5,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    no_cuda=False,
    do_eval=True
)

# Early stopping
# early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)

# Initialize loss tracker
loss_tracker = LossTrackingCallback()

# Initialize trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=dev_dataset,
    callbacks=[loss_tracker],
)

loss_tracker.set_trainer_and_dataset(trainer, test_dataset)

# 8. Train model
print("Starting training...")
trainer.train()
trainer.save_model(base_model_dir)
print("Training completed and model saved.")

# 9. Plot training, validation, and test loss
plt.figure(figsize=(10, 6))
plt.plot(loss_tracker.steps, loss_tracker.train_losses, label='Training Loss')

if loss_tracker.eval_losses and loss_tracker.test_losses:
    eval_step_interval = max(1, len(loss_tracker.steps) // len(loss_tracker.eval_losses))
    eval_steps = [loss_tracker.steps[min(i * eval_step_interval, len(loss_tracker.steps) - 1)] for i in range(len(loss_tracker.eval_losses))]
    plt.plot(eval_steps, loss_tracker.eval_losses, label='Validation Loss')
    plt.plot(eval_steps, loss_tracker.test_losses, label='Test Loss')

plt.xlabel('Steps')
plt.ylabel('Loss')
plt.title('Training, Validation, and Test Loss')
plt.legend()
plt.grid(True)
plt.savefig('loss_curves.png')
plt.show()

# 10. Evaluate on test set
y_pred_classify = trainer.predict(test_dataset)
y_pred_ids = np.argmax(y_pred_classify.predictions, axis=-1)

# Map IDs to text labels
y_pred_text = [label_mapping[idx] for idx in y_pred_ids]
y_true_text = [label_mapping[idx] for idx in test_y]

# Metrics
y_true = test_y

# Confusion Matrix
cf_matrix = confusion_matrix(y_true, y_pred_ids)
plt.figure(figsize=(6, 4))
sn.heatmap(cf_matrix, annot=True, fmt="d", cmap="Blues",
           xticklabels=["negative", "neutral", "positive"],
           yticklabels=["negative", "neutral", "positive"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.tight_layout()
plt.savefig('confusion_matrix.png')
plt.show()

# Scores
acc = accuracy_score(y_true, y_pred_ids)
print("Accuracy: {:.4f}".format(acc))

f1_micro = f1_score(y_true, y_pred_ids, average='micro')
print("F1 - micro: {:.4f}".format(f1_micro))

# f1_macro = f1_score(y_true, y_pred_ids, average='macro')
# print("F1 - macro: {:.4f}".format(f1_macro))

precision = precision_score(y_true, y_pred_ids, average='micro')
print("Precision (micro): {:.4f}".format(precision))

recall = recall_score(y_true, y_pred_ids, average='micro')
print("Recall (micro): {:.4f}".format(recall))

# 11. Plot class-wise metrics
class_names = ["negative", "neutral", "positive"]
class_precision = precision_score(y_true, y_pred_ids, average=None)
class_recall = recall_score(y_true, y_pred_ids, average=None)
class_f1 = f1_score(y_true, y_pred_ids, average=None)

plt.figure(figsize=(8, 6))
x = np.arange(len(class_names))
width = 0.25

plt.bar(x - width, class_precision, width, label='Precision')
plt.bar(x, class_recall, width, label='Recall')
plt.bar(x + width, class_f1, width, label='F1-score')

plt.xlabel('Classes')
plt.ylabel('Score')
plt.title('Class-wise Performance Metrics')
plt.xticks(x, class_names)
plt.legend()
plt.tight_layout()
plt.savefig('vibert4news_class_metrics.png')
plt.show()

# 12. Print some example predictions
print("\nExample predictions:")
print("------------------------")
for i in range(min(10, len(test_X))):
    print(f"Text: {test_X[i][:50]}...")
    print(f"True sentiment: {y_true_text[i]}")
    print(f"Predicted sentiment: {y_pred_text[i]}")
    print("------------------------")

import torch
import re
import pandas as pd
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from underthesea import ner, word_tokenize

# Thiáº¿t bá»‹ xá»­ lÃ½
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 1ï¸âƒ£ Äá»c danh sÃ¡ch tá»« cáº£m xÃºc tá»« file vocab.xlsx
file_path = "/content/drive/MyDrive/NCKH_2425/DATASET/add_data/vocab.xlsx"
df = pd.read_excel(file_path)  # âœ… Äá»c file
object_words = df["Object"].dropna().tolist()

# 2ï¸âƒ£ Táº£i mÃ´ hÃ¬nh PhoBERT phÃ¢n loáº¡i cáº£m xÃºc
sentiment_model_path = "/content/drive/MyDrive/NCKH_2425/ViBert4News/Base"
sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model_path).to("cpu")
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-base-v2", use_fast=False)
sentiment_classifier = pipeline("text-classification", model=sentiment_model, tokenizer=tokenizer, device=-1)


# MÃ´ hÃ¬nh Topic
topic_model_path = "/content/drive/MyDrive/NCKH_2425/PhoBERT/PhoBERT_base/Topic"
topic_model = AutoModelForSequenceClassification.from_pretrained(topic_model_path).to("cpu")
topic_classifier = pipeline("text-classification", model=topic_model, tokenizer=tokenizer, device=-1)

# 3ï¸âƒ£ HÃ m tÃ¬m cÃ¡c tá»« cáº£m xÃºc trong cÃ¢u
def extract_sentiment_words(text, sentiment_words):
    return [word for word in sentiment_words if re.search(r'\b' + re.escape(word) + r'\b', text, re.IGNORECASE)]

# PhÃ¢n tÃ­ch cáº£m xÃºc tá»«ng tá»« trong cÃ¢u
def analyze_words_sentiment(text):
    words = word_tokenize(text, format="text").split()
    word_sentiments = {}

    for word in words:
        inputs = tokenizer(word, return_tensors="pt", truncation=True, padding=True, max_length=512).to("cpu")
        outputs = sentiment_model(**inputs)
        logits = outputs.logits
        prediction = torch.argmax(logits, dim=1).item()

        # Correcting the conditional structure
        if prediction == 2:
            word_sentiments[word] = "Positive"
        elif prediction == 0:
            word_sentiments[word] = "Negative"
        else:
            word_sentiments[word] = "Neutral"

    return word_sentiments


# Nháº­p cÃ¢u Ä‘á»ƒ phÃ¢n tÃ­ch
text = "Tá»¥i em lÃ  táº­p thá»ƒ lá»›p, ngÃ nh Ká»¹ thuáº­t xÃ¢y dá»±ng cá»§a trÆ°á»ng, khÃ³a 55 vÃ  56, kÃ­nh mong Ban GiÃ¡m hiá»‡u, Ban ÄÃ o táº¡o má»Ÿ thÃªm cho tá»¥i em má»™t vÃ i lá»›p há»c pháº§n mÃ´n (Thá»§y lá»±c - Thá»§y vÄƒn vÃ  PhÃ¢n tÃ­ch káº¿t cáº¥u F1). VÃ¬ giá» tá»¥i em lÃ  nÄƒm cuá»‘i cáº£ rá»“i, pháº§n vÃ¬ pháº£i Ä‘Äƒng kÃ½ nhá»¯ng mÃ´n trong há»c ká»³ chÃ­nh, pháº§n vÃ¬ muá»‘n tráº£ ná»£ nhá»¯ng mÃ´n cÃ²n ná»£ hoáº·c chÆ°a Ä‘Äƒng kÃ½ Ä‘Æ°á»£c. Em biáº¿t nguá»“n nhÃ¢n lá»±c giáº£ng dáº¡y cá»§a nhÃ  trÆ°á»ng cÃ²n thiáº¿u vÃ  khÃ´ng Ä‘á»§ Ä‘á»ƒ má»Ÿ thÃªm nhiá»u lá»›p... NhÆ°ng em kÃ­nh mong Ban lÃ£nh Ä‘áº¡o trÆ°á»ng xem xÃ©t má»Ÿ cho tá»¥i em 2 mÃ´n trÃªn. Tá»¥i em cÅ©ng khÃ´ng cÃ²n nhiá»u sá»± lá»±a chá»n ná»¯a vÃ¬ tÃ¢m lÃ½ ai cÅ©ng muá»‘n ra trÆ°á»ng cho Ä‘Ãºng háº¡n, nhÃ  trÆ°á»ng vÃ  Ban ÄÃ o táº¡o thÆ°Æ¡ng tÃ¬nh má»Ÿ cho tá»¥i em thÃªm lá»›p. Em Ä‘Ã£ lÃªn Ban ÄÃ o táº¡o viáº¿t Ä‘Æ¡n 1 láº§n rá»“i nhÆ°ng khÃ´ng cÃ³ cÃ¢u tráº£ lá»i... Em thay máº·t táº­p thá»ƒ ngÃ nh Ká»¹ thuáº­t xÃ¢y dá»±ng K55 vÃ  K56 xin chÃ¢n thÃ nh cáº£m Æ¡n, Ã¢n Ä‘á»©c nÃ y xin ghi nhá»›, kÃ­nh mong admin Ä‘Äƒng riÃªng giÃ¹m em, chÃ¢n thÃ nh cáº£m Æ¡n admin, chÃºc admin sá»©c khá»e, mong admin truyá»n táº£i tÃ¢m tÆ° nguyá»‡n vá»ng nÃ y cho nhÃ  trÆ°á»ng giÃ¹m tá»¥i em"

# PhÃ¢n tÃ­ch cáº£m xÃºc tá»«ng tá»«
# word_sentiments = analyze_words_sentiment(text)
# positive_words_in_text = [word for word, sentiment in word_sentiments.items() if sentiment == "Positive"]
# negative_words_in_text = [word for word, sentiment in word_sentiments.items() if sentiment == "Negative"]

# 5ï¸âƒ£ PhÃ¢n tÃ­ch cáº£m xÃºc toÃ n cÃ¢u
results = sentiment_classifier(text)
predicted_label = results[0]["label"]
predicted_score = results[0]["score"]

attitude_map = {
    "LABEL_0": "Negative",
    "LABEL_1": "Neutral",
    "LABEL_2": "Positive"
}
attitude = attitude_map.get(predicted_label, "Unknown")

# 6ï¸âƒ£ PhÃ¢n tÃ­ch chá»§ Ä‘á»
topic_result = topic_classifier(text)
topic_label = topic_result[0]["label"]
topic_score = topic_result[0]["score"]

topic_map = {
    "LABEL_0": "facility",
    "LABEL_1": "lecturer",
    "LABEL_2": "others",
    "LABEL_3": "training_program"
}
topic = topic_map.get(topic_label, "Unknown")

# 7ï¸âƒ£ NER vÃ  tÃ¬m cÃ¡c tá»« Ä‘á»‘i tÆ°á»£ng
extracted_object_words = extract_sentiment_words(text, object_words)

# ğŸ‘‰ In káº¿t quáº£
print(f"âœ… Predicted attitude: {attitude} (score: {predicted_score:.2f})")
print(f"âœ… Predicted topic: {topic} (score: {topic_score:.2f})")
# print(f"âœ… Positive words: {positive_words_in_text}")
# print(f"âœ… Negative words: {negative_words_in_text}")
# print(f"âœ… Extracted Object Words: {extracted_object_words}")